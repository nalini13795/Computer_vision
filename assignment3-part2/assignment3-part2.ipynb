{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSfnuObtYYMH"
      },
      "source": [
        "# ITCS 4152/5152 Assignment3-Part2\n",
        "**Due date: 11:59 pm EST on Nov 30, 2022 (Wed.)**\n",
        "\n",
        "---\n",
        "In this semester, we will use Google Colab for the assignments, which allows us to utilize resources that some of us might not have in their local machines such as GPUs. You will need to use your UNC Charlotte (*.uncc.edu) account for coding and Google Drive to save your results.\n",
        "\n",
        "## Google Colab Tutorial\n",
        "---\n",
        "Go to https://colab.research.google.com/notebooks/, you will see a tutorial named \"Welcome to Colaboratory\" file, where you can learn the basics of using google colab.\n",
        "\n",
        "Settings used for assignments: ***Edit -> Notebook Settings -> Runtime Type (Python 3)***.\n",
        "\n",
        "## Description\n",
        "---\n",
        "Detection is a fundamental task in CV. This homework is on faster-RCNN, one of the most famous two-stage detection models. You will first implement some basic functions of detection, including generating anchor boxes of different scales and ratios and calculating iou. Then you will train and test on the UAVDT dataset with faster-RCNN ([faster-rcnn.pytorch](https://github.com/jwyang/faster-rcnn.pytorch)) to see how a detection model really works.\n",
        "\n",
        "The UAVDT dataset (\"data_uav.tar.gz\") and the pretrained faster-RCNN model (\"data/pretrained_model/vgg16_caffe.pth\") are in https://drive.google.com/drive/folders/1zEDPAKjv2gk2ivhsLZlMDVgCqI0se-lN?usp=sharing. Please copy the \"data_uav.tar.gz\" to \"assignment3-part2/\" and copy the \"data\" folder to \"assignment3-part2/faster-rcnn.pytorch/\".\n",
        "\n",
        "The code of this homework strictly relies on the right version of some libraries. DO NOT change anything except cells you are required to fill in. \n",
        "\n",
        "## Setup Environment\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCoZ4NhzAfOd"
      },
      "outputs": [],
      "source": [
        "# remember to RESTART runtime when you see prompt asking to do so\n",
        "!pip install torch==1.3.0+cu100 torchvision==0.4.1+cu100 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install Pillow==9.0.0\n",
        "!pip install scipy==1.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g01x8yQKBtGM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThW7hYKSiMrh"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import scipy.sparse\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7T72O-1ks-a"
      },
      "outputs": [],
      "source": [
        "# Mount your google drive where you've saved your assignment folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2s75J7ApdTg"
      },
      "outputs": [],
      "source": [
        "# Replace '------' with the path such that \"ITCS_4152_5152_assignment3-part2\" is your working directory\n",
        "%cd '/content/gdrive/MyDrive/CV/Katiyar_Nalini_801204572_assignment3_part2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrJQLGBZUkIi"
      },
      "outputs": [],
      "source": [
        "!tar -xvf data_uav.tar.gz -C faster-rcnn.pytorch/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd faster-rcnn.pytorch"
      ],
      "metadata": {
        "id": "VG7_Brzrt4jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPBBVxo76FZj"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hchFpOrO6PHT"
      },
      "outputs": [],
      "source": [
        "%cd lib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4QwdbPyDBH7"
      },
      "outputs": [],
      "source": [
        "!rm -rf build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bsxp--H8NRI5"
      },
      "outputs": [],
      "source": [
        "!python setup.py build develop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LtyCvwSOthm"
      },
      "outputs": [],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y68s6GN-3Wfi",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# get ground-truth annotation\n",
        "\n",
        "def load_pascal_annotation(data_path, index, num_classes):\n",
        "  \"\"\"\n",
        "  Load image and bounding boxes info from XML file in the PASCAL VOC\n",
        "  format.\n",
        "  \"\"\"\n",
        "  filename = os.path.join(data_path, 'Annotations', index + '.xml')\n",
        "  tree = ET.parse(filename)\n",
        "  objs = tree.findall('object')\n",
        "\n",
        "  # Exclude the samples labeled as difficult\n",
        "  non_diff_objs = [\n",
        "      obj for obj in objs if int(obj.find('difficult').text) == 0]\n",
        "  if len(non_diff_objs) != len(objs):\n",
        "      print('Removed {} difficult objects'.format(\n",
        "          len(objs) - len(non_diff_objs)))\n",
        "  objs = non_diff_objs\n",
        "  num_objs = len(objs)\n",
        "\n",
        "  boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n",
        "  gt_classes = [i for i in range(num_objs)]\n",
        "\n",
        "  # Load object bounding boxes into a data frame.\n",
        "  for ix, obj in enumerate(objs):\n",
        "      bbox = obj.find('bndbox')\n",
        "      # Make pixel indexes 0-based\n",
        "      x1 = float(bbox.find('xmin').text) - 1\n",
        "      y1 = float(bbox.find('ymin').text) - 1\n",
        "      x2 = float(bbox.find('xmax').text) - 1\n",
        "      y2 = float(bbox.find('ymax').text) - 1\n",
        "      cls = obj.find('name').text.lower().strip()\n",
        "      boxes[ix, :] = [x1, y1, x2, y2]\n",
        "      gt_classes[ix] = cls\n",
        "\n",
        "  return {'boxes' : boxes,\n",
        "          'gt_classes': gt_classes}\n",
        "\n",
        "# plot bounding box on image\n",
        "def vis_detections(im, bbox, color=(0, 204, 0), clip=None, width=2):\n",
        "  im_ = np.copy(im)\n",
        "  for i in range(len(bbox)):\n",
        "    cv2.rectangle(im_, (int(bbox[i][0]),int(bbox[i][1])), (int(bbox[i][2]),int(bbox[i][3])), color, width)\n",
        "\n",
        "  # Be sure to convert the color space of the image from\n",
        "  # BGR (Opencv) to RGB (Matplotlib) before you show a \n",
        "  # color image read from OpenCV\n",
        "  plt.figure(figsize=(20, 10));\n",
        "  plt.subplot(1, 2, 1);\n",
        "  if clip is not None:\n",
        "    plt.imshow(cv2.cvtColor(im_[clip[0]:clip[2],clip[1]:clip[3],:], cv2.COLOR_BGR2RGB));\n",
        "  else:\n",
        "    plt.imshow(cv2.cvtColor(im_, cv2.COLOR_BGR2RGB));\n",
        "  plt.title('train image (bounding box)');\n",
        "  plt.axis(\"off\");\n",
        "  \n",
        "  return im_\n",
        "\n",
        "# read the image for local directory (same with this .ipynb) \n",
        "img = cv2.imread('images/img0.jpg')\n",
        "\n",
        "data_path = 'data/VOCdevkit2007/UAV2017/'\n",
        "index = 'M0101_1'\n",
        "gt = load_pascal_annotation(data_path, index, 2)\n",
        "\n",
        "print(\"{} ground-truth bounding boxes in all\".format(len(gt['boxes'])))\n",
        "for _i in range(len(gt['boxes'])):\n",
        "  print('bbox{}: {} class: {}'.format(_i, gt['boxes'][_i],gt['gt_classes'][_i]))\n",
        "\n",
        "img_gt = vis_detections(img, gt['boxes'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbF88rI5ww_j"
      },
      "source": [
        "## Problem 1: Basic functions \n",
        "(10 points + 10 bonus points) You will first implement two basic functions in detection, generating anchor boxes of different scales and ratios. Then you will calculate the Intersection over Union (iou) between the ground-truth bounding boxes and your generated anchor boxes. Finally, you will do ROI pooling to get feature map of fixed size. The ROI pooling part is a bonus question of 10 points. For more information about anchor boxes and iou, you can refer to https://d2l.ai/chapter_computer-vision/anchor.html or other online materials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eb1VSN_uUxRR"
      },
      "outputs": [],
      "source": [
        "# generate anchor boxes of different scales and ratios\n",
        "def generate_anchors(center, scales, ratios, base_size=4):  \n",
        "    # base_size is your anchor box size when scale=1 and ratio=1\n",
        "    ##########--WRITE YOUR CODE HERE--########## \n",
        "    scales = torch.tensor(scales)\n",
        "    ratios = torch.tensor(ratios)\n",
        "    size_ratios = base_size/ratios\n",
        "    ws = np.round(np.sqrt(size_ratios))\n",
        "    hs = np.round(ws * ratios)\n",
        "    ws = ws[:, np.newaxis]\n",
        "    hs = hs[:, np.newaxis]\n",
        "    ratio_anchors = np.hstack((center[0] - 0.5 * (ws - 1),\n",
        "                         center[1] - 0.5 * (hs - 1),\n",
        "                         center[0] + 0.5 * (ws - 1),\n",
        "                         center[1] + 0.5 * (hs - 1)))\n",
        "    anchors = []\n",
        "    for i in range(ratio_anchors.shape[0]):\n",
        "      w, h, x_ctr, y_ctr = _whctrs(ratio_anchors[i, :])\n",
        "      ws = w * scales\n",
        "      hs = h * scales\n",
        "      anchors_add = _mkanchors(ws, hs, x_ctr, y_ctr)\n",
        "      anchors.append(anchors_add)\n",
        "\n",
        "    anchors = torch.tensor(anchors)\n",
        "    # print(anchors.shape)\n",
        "    # anchors = torch.reshape(anchors, (4,1))\n",
        "    # print('anc1',anchors)\n",
        "    # anchors.unsqueeze(0)\n",
        "    # anchors.unsqueeze(0)\n",
        "    # print(anchors.unsqueeze(0))\n",
        "    # print(anchors[0])\n",
        "    ##########-------END OF CODE-------########## \n",
        "    return anchors\n",
        "\n",
        "def _mkanchors(ws, hs, x_ctr, y_ctr):\n",
        "    \"\"\"\n",
        "    Given a vector of widths (ws) and heights (hs) around a center\n",
        "    (x_ctr, y_ctr), output a set of anchors (windows).\n",
        "    \"\"\"\n",
        "\n",
        "    ws = ws[:, np.newaxis]\n",
        "    hs = hs[:, np.newaxis]\n",
        "    anchors = np.hstack((x_ctr - 0.5 * (ws - 1),\n",
        "                         y_ctr - 0.5 * (hs - 1),\n",
        "                         x_ctr + 0.5 * (ws - 1),\n",
        "                         y_ctr + 0.5 * (hs - 1)))\n",
        "    return anchors\n",
        "\n",
        "def _scale_enum(anchor,scales):\n",
        "    \"\"\"\n",
        "    Enumerate a set of anchors for each scale wrt an anchor.\n",
        "    \n",
        "    Inputs:\n",
        "        anchor - Transformed tensor using ratios, form - [x_tl,y_tl,x_br,y_br]\n",
        "        scales - 1D tensor , default - torch.tensor([8,16,32])\n",
        "    Output:\n",
        "        anchors - Shape - scales.shape[0] x 4\n",
        "    \"\"\"\n",
        "    w,h,x_ctr,y_ctr=_whctrs(anchor)\n",
        "    ws=w*scales \n",
        "    hs=h*scales \n",
        "    #ws and hs shape - scales.shape[0]\n",
        "    anchors=_mkanchors(ws,hs,x_ctr,y_ctr)\n",
        "    return anchors\n",
        "\n",
        "def _whctrs(anchor):\n",
        "    \"\"\"\n",
        "    Return width, height, x center, and y center for an anchor (window).\n",
        "    \"\"\"\n",
        "\n",
        "    w = anchor[2] - anchor[0] + 1\n",
        "    h = anchor[3] - anchor[1] + 1\n",
        "    x_ctr = anchor[0] + 0.5 * (w - 1)\n",
        "    y_ctr = anchor[1] + 0.5 * (h - 1)\n",
        "    return w, h, x_ctr, y_ctr\n",
        "\n",
        "center = [190, 170]\n",
        "anchor_scales = [8, 16, 32]\n",
        "anchor_ratios = [0.5, 1, 2]\n",
        "bbox = generate_anchors(center, anchor_scales, anchor_ratios)\n",
        "# crop image for better view\n",
        "im_ = np.copy(img_gt)\n",
        "for anchor in bbox:\n",
        "  img_gt_a = vis_detections(im_, anchor, (0,0,255), (0,0,300,400))\n",
        "  im_ = img_gt_a\n",
        "# plt.imshow(img_gt)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6iGnl8yuOyu"
      },
      "outputs": [],
      "source": [
        "def bbox_iou(bbox_a, bbox_b):\n",
        "    \"\"\"Calculate the Intersection of Unions (IoUs) between bounding boxes.\n",
        "\n",
        "    IoU is calculated as a ratio of area of the intersection\n",
        "    and area of the union.\n",
        "\n",
        "    both inputs and output should be python list\n",
        "    \"\"\"\n",
        "    ##########--WRITE YOUR CODE HERE--##########\n",
        "    iou = []\n",
        "    bbox_b = bbox_b.numpy()\n",
        "    bbox_a = bbox_a[0]\n",
        "    for bbox in bbox_b:\n",
        "      for box in bbox: \n",
        "        xA = max(bbox_a[0], box[0])\n",
        "        yA = max(bbox_a[1], box[1])\n",
        "        xB = min(bbox_a[2], box[2])\n",
        "        yB = min(bbox_a[3], box[3])\n",
        "        # compute the area of intersection rectangle\n",
        "        interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
        "        # compute the area of both the prediction and ground-truth\n",
        "        # rectangles\n",
        "        bbox_aArea = (bbox_a[2] - bbox_a[0] + 1) * (bbox_a[3] - bbox_a[1] + 1)\n",
        "        bboxArea = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)\n",
        "        # compute the intersection over union by taking the intersection\n",
        "        # area and dividing it by the sum of prediction + ground-truth\n",
        "        # areas - the interesection area\n",
        "        iou_1 = interArea / float(bbox_aArea + bboxArea - interArea)\n",
        "        iou.append(iou_1)\n",
        "    ##########-------END OF CODE-------########## \n",
        "    return iou\n",
        "\n",
        "bbox_gt = [[141,147,247,192]]\n",
        "iou = bbox_iou(bbox_gt, bbox)\n",
        "bbox_filter = []\n",
        "iou_max = 0.0\n",
        "bbox_max = None\n",
        "bbox = bbox.numpy()\n",
        "print(iou)\n",
        "i = -1\n",
        "for box_ in bbox:\n",
        "  for bbox_ in box_:\n",
        "    # get bbox of maximum iou\n",
        "    i+=1\n",
        "    if iou[i] > iou_max:\n",
        "      iou_max = iou[i]\n",
        "      bbox_max = bbox_\n",
        "    # only keep bbox of high iou as postive and low iou as negative\n",
        "    if iou[i]>=0.25 and iou[i]<=0.7:\n",
        "      continue\n",
        "    print(\"gt:{}  bbox:{}  iou:{:.4f}\".format(bbox_gt[0], bbox_, iou[i]))\n",
        "    bbox_filter.append(bbox_)\n",
        "\n",
        "img_gt_b = vis_detections(img_gt, bbox_filter, (0,0,255), (0,0,300,400))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXbxOcJM7xWD"
      },
      "outputs": [],
      "source": [
        "print(\"gt:{}  bbox_max:{}  iou_max:{:.4f}\".format(bbox_gt[0], bbox_max, iou_max))\n",
        "\n",
        "def roi_pooling(roi, pooling_shape):\n",
        "  ##########--WRITE YOUR CODE HERE--##########\n",
        "  # check the shape of roi first and pay attention to the channel dimension\n",
        "  # Use numpy for the implementation in this block.\n",
        "  # DON'T use torch or torchvision in this block.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ##########-------END OF CODE-------##########\n",
        "  return roip\n",
        "\n",
        "# pooling on anchor box of maximum iou\n",
        "roi = img[bbox_max[1]:bbox_max[3],bbox_max[0]:bbox_max[2],:]\n",
        "roip = roi_pooling(roi, (25,25)) \n",
        "print(roip.shape)\n",
        "\n",
        "# pooling on whole image\n",
        "imgp = roi_pooling(img, (25,25)) \n",
        "print(imgp.shape)\n",
        "\n",
        "plt.figure(figsize=(16, 8));\n",
        "plt.subplot(1, 2, 1);\n",
        "plt.imshow(cv2.cvtColor(roip, cv2.COLOR_BGR2RGB));\n",
        "plt.subplot(1, 2, 2);\n",
        "plt.imshow(cv2.cvtColor(imgp, cv2.COLOR_BGR2RGB));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx8pbgK9zjaM"
      },
      "source": [
        "## Problem 2: Faster-RCNN\n",
        "\n",
        "(10 points) Unmanned Aerial Vehicles (UAVs) are used to fuel numerous important applications in computer vision. This UAVDT dataset contains several videos of traffic. Each video is provided as a sequence of JPEG images. It only has one class of object, i.e., car. The subset contains only two videos for training and one video for testing, just to reduce time.  \n",
        "\n",
        "You will train and test Faster-RCNN on the small subset. Actually, this part is so complicated that you **do not** have the opportunity to write any code. All you need is to run this section, output the correct detection result images, and make discussions. However, you are stil strongly recommended to understand the function of each part. You might encounter some strange errors, and in which situation, I would suggest you to check that the dataset has been successful extracted on google drive and restart your runtime.\n",
        "\n",
        "Remember that you need to finish the **discussion** at the last part."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "from easydict import EasyDict as edict\n",
        "filename = 'cfgs/vgg16.yml'\n",
        "with open(filename, 'r') as f:\n",
        "  yaml_cfg = edict(yaml.load(f,Loader=yaml.SafeLoader))\n",
        "\n",
        "print(yaml_cfg)"
      ],
      "metadata": {
        "id": "QcLLALBWNS6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCd0C_REtYKk"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------------\n",
        "# Pytorch multi-GPU Faster R-CNN\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Written by Jiasen Lu, Jianwei Yang, based on code from Ross Girshick\n",
        "# Modified by Tao for HW5\n",
        "# --------------------------------------------------------\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import _init_paths\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import argparse\n",
        "import pprint\n",
        "import pdb\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import Sampler\n",
        "\n",
        "from roi_data_layer.roidb import combined_roidb\n",
        "from roi_data_layer.roibatchLoader import roibatchLoader\n",
        "from model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\n",
        "from model.utils.net_utils import weights_normal_init, save_net, load_net, \\\n",
        "      adjust_learning_rate, save_checkpoint, clip_gradient\n",
        "\n",
        "from model.faster_rcnn.vgg16 import vgg16\n",
        "from model.faster_rcnn.resnet import resnet\n",
        "\n",
        "def parse_args(args):\n",
        "  \"\"\"\n",
        "  Parse input arguments\n",
        "  \"\"\"\n",
        "  parser = argparse.ArgumentParser(description='Train a Fast R-CNN network')\n",
        "  parser.add_argument('--dataset', dest='dataset',\n",
        "                      help='training dataset',\n",
        "                      default='pascal_voc', type=str)\n",
        "  parser.add_argument('--net', dest='net',\n",
        "                    help='vgg16, res101',\n",
        "                    default='vgg16', type=str)\n",
        "  parser.add_argument('--start_epoch', dest='start_epoch',\n",
        "                      help='starting epoch',\n",
        "                      default=1, type=int)\n",
        "  parser.add_argument('--epochs', dest='max_epochs',\n",
        "                      help='number of epochs to train',\n",
        "                      default=20, type=int)\n",
        "  parser.add_argument('--disp_interval', dest='disp_interval',\n",
        "                      help='number of iterations to display',\n",
        "                      default=100, type=int)\n",
        "  parser.add_argument('--checkpoint_interval', dest='checkpoint_interval',\n",
        "                      help='number of iterations to display',\n",
        "                      default=10000, type=int)\n",
        "\n",
        "  parser.add_argument('--save_dir', dest='save_dir',\n",
        "                      help='directory to save models', default=\"models\",\n",
        "                      type=str)\n",
        "  parser.add_argument('--nw', dest='num_workers',\n",
        "                      help='number of workers to load data',\n",
        "                      default=0, type=int)\n",
        "  parser.add_argument('--cuda', dest='cuda',\n",
        "                      help='whether use CUDA',\n",
        "                      action='store_true')\n",
        "  parser.add_argument('--ls', dest='large_scale',\n",
        "                      help='whether use large imag scale',\n",
        "                      action='store_true')                      \n",
        "  parser.add_argument('--mGPUs', dest='mGPUs',\n",
        "                      help='whether use multiple GPUs',\n",
        "                      action='store_true')\n",
        "  parser.add_argument('--bs', dest='batch_size',\n",
        "                      help='batch_size',\n",
        "                      default=1, type=int)\n",
        "  parser.add_argument('--cag', dest='class_agnostic',\n",
        "                      help='whether to perform class_agnostic bbox regression',\n",
        "                      action='store_true')\n",
        "\n",
        "# config optimization\n",
        "  parser.add_argument('--o', dest='optimizer',\n",
        "                      help='training optimizer',\n",
        "                      default=\"sgd\", type=str)\n",
        "  parser.add_argument('--lr', dest='lr',\n",
        "                      help='starting learning rate',\n",
        "                      default=0.001, type=float)\n",
        "  parser.add_argument('--lr_decay_step', dest='lr_decay_step',\n",
        "                      help='step to do learning rate decay, unit is epoch',\n",
        "                      default=5, type=int)\n",
        "  parser.add_argument('--lr_decay_gamma', dest='lr_decay_gamma',\n",
        "                      help='learning rate decay ratio',\n",
        "                      default=0.1, type=float)\n",
        "\n",
        "# set training session\n",
        "  parser.add_argument('--s', dest='session',\n",
        "                      help='training session',\n",
        "                      default=1, type=int)\n",
        "\n",
        "# resume trained model\n",
        "  parser.add_argument('--r', dest='resume',\n",
        "                      help='resume checkpoint or not',\n",
        "                      default=False, type=bool)\n",
        "  parser.add_argument('--checksession', dest='checksession',\n",
        "                      help='checksession to load model',\n",
        "                      default=1, type=int)\n",
        "  parser.add_argument('--checkepoch', dest='checkepoch',\n",
        "                      help='checkepoch to load model',\n",
        "                      default=1, type=int)\n",
        "  parser.add_argument('--checkpoint', dest='checkpoint',\n",
        "                      help='checkpoint to load model',\n",
        "                      default=0, type=int)\n",
        "# log and display\n",
        "  parser.add_argument('--use_tfb', dest='use_tfboard',\n",
        "                      help='whether use tensorboard',\n",
        "                      action='store_true')\n",
        "\n",
        "  args = parser.parse_args(args)\n",
        "\n",
        "  return args\n",
        "\n",
        "\n",
        "class sampler(Sampler):\n",
        "  def __init__(self, train_size, batch_size):\n",
        "    self.num_data = train_size\n",
        "    self.num_per_batch = int(train_size / batch_size)\n",
        "    self.batch_size = batch_size\n",
        "    self.range = torch.arange(0,batch_size).view(1, batch_size).long()\n",
        "    self.leftover_flag = False\n",
        "    if train_size % batch_size:\n",
        "      self.leftover = torch.arange(self.num_per_batch*batch_size, train_size).long()\n",
        "      self.leftover_flag = True\n",
        "\n",
        "  def __iter__(self):\n",
        "    rand_num = torch.randperm(self.num_per_batch).view(-1,1) * self.batch_size\n",
        "    self.rand_num = rand_num.expand(self.num_per_batch, self.batch_size) + self.range\n",
        "\n",
        "    self.rand_num_view = self.rand_num.view(-1)\n",
        "\n",
        "    if self.leftover_flag:\n",
        "      self.rand_num_view = torch.cat((self.rand_num_view, self.leftover),0)\n",
        "\n",
        "    return iter(self.rand_num_view)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.num_data\n",
        "\n",
        "\n",
        "# add your args here\n",
        "args = parse_args(['--dataset','uav','--net','vgg16','--bs','8','--lr','1e-2','--lr_decay_step','4','--epochs','2','--cuda'])\n",
        "\n",
        "print('Called with args:')\n",
        "print(args)\n",
        "\n",
        "args.imdb_name = \"uav_2017_trainval\"\n",
        "args.imdbval_name = \"uav_2017_test\"\n",
        "args.set_cfgs = ['ANCHOR_SCALES', '[8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]', 'MAX_NUM_GT_BOXES', '20']\n",
        "\n",
        "args.cfg_file = \"cfgs/{}_ls.yml\".format(args.net) if args.large_scale else \"cfgs/{}.yml\".format(args.net)\n",
        "\n",
        "print(args.cfg_file)\n",
        "if args.cfg_file is not None:\n",
        "  cfg_from_file(args.cfg_file)\n",
        "if args.set_cfgs is not None:\n",
        "  cfg_from_list(args.set_cfgs)\n",
        "\n",
        "print('Using config:')\n",
        "pprint.pprint(cfg)\n",
        "np.random.seed(cfg.RNG_SEED)\n",
        "\n",
        "#torch.backends.cudnn.benchmark = True\n",
        "if torch.cuda.is_available() and not args.cuda:\n",
        "  print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "# train set\n",
        "# -- Note: Use validation set and disable the flipped to enable faster loading.\n",
        "cfg.TRAIN.USE_FLIPPED = False\n",
        "cfg.USE_GPU_NMS = args.cuda\n",
        "imdb, roidb, ratio_list, ratio_index = combined_roidb(args.imdb_name)\n",
        "train_size = len(roidb)\n",
        "\n",
        "print('{:d} roidb entries'.format(len(roidb)))\n",
        "\n",
        "output_dir = args.save_dir + \"/\" + args.net + \"/\" + args.dataset\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)\n",
        "\n",
        "sampler_batch = sampler(train_size, args.batch_size)\n",
        "\n",
        "dataset = roibatchLoader(roidb, ratio_list, ratio_index, args.batch_size, \\\n",
        "                          imdb.num_classes, training=True)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size,\n",
        "                          sampler=sampler_batch, num_workers=args.num_workers)\n",
        "\n",
        "# initilize the tensor holder here.\n",
        "im_data = torch.FloatTensor(1)\n",
        "im_info = torch.FloatTensor(1)\n",
        "num_boxes = torch.LongTensor(1)\n",
        "gt_boxes = torch.FloatTensor(1)\n",
        "\n",
        "# ship to cuda\n",
        "if args.cuda:\n",
        "  im_data = im_data.cuda()\n",
        "  im_info = im_info.cuda()\n",
        "  num_boxes = num_boxes.cuda()\n",
        "  gt_boxes = gt_boxes.cuda()\n",
        "\n",
        "# make variable\n",
        "im_data = Variable(im_data)\n",
        "im_info = Variable(im_info)\n",
        "num_boxes = Variable(num_boxes)\n",
        "gt_boxes = Variable(gt_boxes)\n",
        "\n",
        "if args.cuda:\n",
        "  cfg.CUDA = True\n",
        "\n",
        "# initilize the network here.\n",
        "if args.net == 'vgg16':\n",
        "  fasterRCNN = vgg16(imdb.classes, pretrained=True, class_agnostic=args.class_agnostic)\n",
        "elif args.net == 'res101':\n",
        "  fasterRCNN = resnet(imdb.classes, 101, pretrained=True, class_agnostic=args.class_agnostic)\n",
        "elif args.net == 'res50':\n",
        "  fasterRCNN = resnet(imdb.classes, 50, pretrained=True, class_agnostic=args.class_agnostic)\n",
        "elif args.net == 'res152':\n",
        "  fasterRCNN = resnet(imdb.classes, 152, pretrained=True, class_agnostic=args.class_agnostic)\n",
        "else:\n",
        "  print(\"network is not defined\")\n",
        "  pdb.set_trace()\n",
        "\n",
        "fasterRCNN.create_architecture()\n",
        "\n",
        "lr = cfg.TRAIN.LEARNING_RATE\n",
        "lr = args.lr\n",
        "\n",
        "params = []\n",
        "for key, value in dict(fasterRCNN.named_parameters()).items():\n",
        "  if value.requires_grad:\n",
        "    if 'bias' in key:\n",
        "      params += [{'params':[value],'lr':lr*(cfg.TRAIN.DOUBLE_BIAS + 1), \\\n",
        "              'weight_decay': cfg.TRAIN.BIAS_DECAY and cfg.TRAIN.WEIGHT_DECAY or 0}]\n",
        "    else:\n",
        "      params += [{'params':[value],'lr':lr, 'weight_decay': cfg.TRAIN.WEIGHT_DECAY}]\n",
        "\n",
        "if args.optimizer == \"adam\":\n",
        "  lr = lr * 0.1\n",
        "  optimizer = torch.optim.Adam(params)\n",
        "\n",
        "elif args.optimizer == \"sgd\":\n",
        "  optimizer = torch.optim.SGD(params, momentum=cfg.TRAIN.MOMENTUM)\n",
        "\n",
        "if args.cuda:\n",
        "  fasterRCNN.cuda()\n",
        "\n",
        "if args.mGPUs:\n",
        "  fasterRCNN = nn.DataParallel(fasterRCNN)\n",
        "\n",
        "iters_per_epoch = int(train_size / args.batch_size)\n",
        "\n",
        "if args.use_tfboard:\n",
        "  from tensorboardX import SummaryWriter\n",
        "  logger = SummaryWriter(\"logs\")\n",
        "\n",
        "for epoch in range(args.start_epoch, args.max_epochs + 1):\n",
        "  # setting to train mode\n",
        "  fasterRCNN.train()\n",
        "  loss_temp = 0\n",
        "  start = time.time()\n",
        "\n",
        "  if epoch % (args.lr_decay_step + 1) == 0:\n",
        "      adjust_learning_rate(optimizer, args.lr_decay_gamma)\n",
        "      lr *= args.lr_decay_gamma\n",
        "\n",
        "  data_iter = iter(dataloader)\n",
        "  for step in range(iters_per_epoch):\n",
        "    data = next(data_iter)\n",
        "    im_data.resize_(data[0].size()).copy_(data[0])\n",
        "    im_info.resize_(data[1].size()).copy_(data[1])\n",
        "    gt_boxes.resize_(data[2].size()).copy_(data[2])\n",
        "    num_boxes.resize_(data[3].size()).copy_(data[3])\n",
        "\n",
        "    fasterRCNN.zero_grad()\n",
        "    rois, cls_prob, bbox_pred, \\\n",
        "    rpn_loss_cls, rpn_loss_box, \\\n",
        "    RCNN_loss_cls, RCNN_loss_bbox, \\\n",
        "    rois_label = fasterRCNN(im_data, im_info, gt_boxes, num_boxes)\n",
        "\n",
        "    loss = rpn_loss_cls.mean() + rpn_loss_box.mean() \\\n",
        "          + RCNN_loss_cls.mean() + RCNN_loss_bbox.mean()\n",
        "    loss_temp += loss.item()\n",
        "\n",
        "    # backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    if args.net == \"vgg16\":\n",
        "        clip_gradient(fasterRCNN, 10.)\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % args.disp_interval == 0:\n",
        "      end = time.time()\n",
        "      if step > 0:\n",
        "        loss_temp /= (args.disp_interval + 1)\n",
        "\n",
        "      if args.mGPUs:\n",
        "        loss_rpn_cls = rpn_loss_cls.mean().item()\n",
        "        loss_rpn_box = rpn_loss_box.mean().item()\n",
        "        loss_rcnn_cls = RCNN_loss_cls.mean().item()\n",
        "        loss_rcnn_box = RCNN_loss_bbox.mean().item()\n",
        "        fg_cnt = torch.sum(rois_label.data.ne(0))\n",
        "        bg_cnt = rois_label.data.numel() - fg_cnt\n",
        "      else:\n",
        "        loss_rpn_cls = rpn_loss_cls.item()\n",
        "        loss_rpn_box = rpn_loss_box.item()\n",
        "        loss_rcnn_cls = RCNN_loss_cls.item()\n",
        "        loss_rcnn_box = RCNN_loss_bbox.item()\n",
        "        fg_cnt = torch.sum(rois_label.data.ne(0))\n",
        "        bg_cnt = rois_label.data.numel() - fg_cnt\n",
        "\n",
        "      print(\"[session %d][epoch %2d][iter %4d/%4d] loss: %.4f, lr: %.2e\" \\\n",
        "                              % (args.session, epoch, step, iters_per_epoch, loss_temp, lr))\n",
        "      print(\"\\t\\t\\tfg/bg=(%d/%d), time cost: %f\" % (fg_cnt, bg_cnt, end-start))\n",
        "      print(\"\\t\\t\\trpn_cls: %.4f, rpn_box: %.4f, rcnn_cls: %.4f, rcnn_box %.4f\" \\\n",
        "                    % (loss_rpn_cls, loss_rpn_box, loss_rcnn_cls, loss_rcnn_box))\n",
        "      if args.use_tfboard:\n",
        "        info = {\n",
        "          'loss': loss_temp,\n",
        "          'loss_rpn_cls': loss_rpn_cls,\n",
        "          'loss_rpn_box': loss_rpn_box,\n",
        "          'loss_rcnn_cls': loss_rcnn_cls,\n",
        "          'loss_rcnn_box': loss_rcnn_box\n",
        "        }\n",
        "        logger.add_scalars(\"logs_s_{}/losses\".format(args.session), info, (epoch - 1) * iters_per_epoch + step)\n",
        "\n",
        "      loss_temp = 0\n",
        "      start = time.time()\n",
        "\n",
        "  \n",
        "  save_name = os.path.join(output_dir, 'faster_rcnn_{}_{}_{}.pth'.format(args.session, epoch, step))\n",
        "  save_checkpoint({\n",
        "    'session': args.session,\n",
        "    'epoch': epoch + 1,\n",
        "    'model': fasterRCNN.module.state_dict() if args.mGPUs else fasterRCNN.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    'pooling_mode': cfg.POOLING_MODE,\n",
        "    'class_agnostic': args.class_agnostic,\n",
        "  }, save_name)\n",
        "  print('save model: {}'.format(save_name))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls\n"
      ],
      "metadata": {
        "id": "8gKbw34ZTwDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B_TtCUD84Ys"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------------\n",
        "# Tensorflow Faster R-CNN\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# Written by Jiasen Lu, Jianwei Yang, based on code from Ross Girshick\n",
        "# Modified by Tao for HW5\n",
        "# --------------------------------------------------------\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import _init_paths\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import argparse\n",
        "import pprint\n",
        "import pdb\n",
        "import time\n",
        "import cv2\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "from roi_data_layer.roidb import combined_roidb\n",
        "from roi_data_layer.roibatchLoader import roibatchLoader\n",
        "from model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\n",
        "from model.rpn.bbox_transform import clip_boxes\n",
        "#from model.nms.nms_wrapper import nms\n",
        "from model.roi_layers import nms\n",
        "from model.rpn.bbox_transform import bbox_transform_inv\n",
        "from model.utils.net_utils import save_net, load_net, vis_detections\n",
        "from model.faster_rcnn.vgg16 import vgg16\n",
        "from model.faster_rcnn.resnet import resnet\n",
        "\n",
        "\n",
        "try:\n",
        "    xrange  # Python 2\n",
        "except NameError:\n",
        "    xrange = range  # Python 3\n",
        "\n",
        "\n",
        "def parse_args(args):\n",
        "    \"\"\"\n",
        "    Parse input arguments\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser(description='Train a Fast R-CNN network')\n",
        "    parser.add_argument('--dataset', dest='dataset',\n",
        "                        help='training dataset',\n",
        "                        default='uav', type=str)\n",
        "    parser.add_argument('--cfg', dest='cfg_file',\n",
        "                        help='optional config file',\n",
        "                        default='cfgs/vgg16.yml', type=str)\n",
        "    parser.add_argument('--net', dest='net',\n",
        "                        help='vgg16, res50, res101, res152',\n",
        "                        default='res101', type=str)\n",
        "    parser.add_argument('--set', dest='set_cfgs',\n",
        "                        help='set config keys', default=None,\n",
        "                        nargs=argparse.REMAINDER)\n",
        "    parser.add_argument('--save_dir', dest='save_dir',\n",
        "                        help='directory to save models', default=\"models\",\n",
        "                        type=str)\n",
        "    parser.add_argument('--cuda', dest='cuda',\n",
        "                        help='whether use CUDA',\n",
        "                        action='store_true')\n",
        "    parser.add_argument('--ls', dest='large_scale',\n",
        "                        help='whether use large imag scale',\n",
        "                        action='store_true')\n",
        "    parser.add_argument('--mGPUs', dest='mGPUs',\n",
        "                        help='whether use multiple GPUs',\n",
        "                        action='store_true')\n",
        "    parser.add_argument('--cag', dest='class_agnostic',\n",
        "                        help='whether perform class_agnostic bbox regression',\n",
        "                        action='store_true')\n",
        "    parser.add_argument('--parallel_type', dest='parallel_type',\n",
        "                        help='which part of model to parallel, 0: all, 1: model before roi pooling',\n",
        "                        default=0, type=int)\n",
        "    parser.add_argument('--checksession', dest='checksession',\n",
        "                        help='checksession to load model',\n",
        "                        default=1, type=int)\n",
        "    parser.add_argument('--checkepoch', dest='checkepoch',\n",
        "                        help='checkepoch to load network',\n",
        "                        default=1, type=int)\n",
        "    parser.add_argument('--checkpoint', dest='checkpoint',\n",
        "                        help='checkpoint to load network',\n",
        "                        default=3960, type=int)\n",
        "    parser.add_argument('--vis', dest='vis',\n",
        "                        help='visualization mode',\n",
        "                        action='store_true')\n",
        "    parser.add_argument('--model_dir', dest='model_dir',\n",
        "                        help='directory to save models', default=\"models\",\n",
        "                        type=str)\n",
        "    parser.add_argument('--use_restarting', dest='use_restarting',\n",
        "                        help='where to use restarting',\n",
        "                        action='store_true')\n",
        "    parser.add_argument('--is_baseline_method', dest='is_baseline_method',\n",
        "                        help='whether to evaluate the baseline method',\n",
        "                        action='store_true')\n",
        "    parser.add_argument('--ovthresh', dest='ovthresh',\n",
        "                        help='the IoU threshold for evaluation',\n",
        "                        default=0.7, type=float)\n",
        "    parser.add_argument('--overall_eval', dest='overall_eval',\n",
        "                        help='display the evaluation results regularly',\n",
        "                        action='store_true')\n",
        "\n",
        "    args = parser.parse_args(args)\n",
        "    return args\n",
        "\n",
        "\n",
        "lr = cfg.TRAIN.LEARNING_RATE\n",
        "momentum = cfg.TRAIN.MOMENTUM\n",
        "weight_decay = cfg.TRAIN.WEIGHT_DECAY\n",
        "\n",
        "\n",
        "\n",
        "args = parse_args(['--dataset','uav','--net','vgg16','--checkepoch','2','--checkpoint','110','--cuda'])\n",
        "\n",
        "args.model_dir = args.model_dir + \"/\" + args.net + \"/\" + args.dataset\n",
        "args.is_baseline_method = True\n",
        "\n",
        "print('Called with args:')\n",
        "print(args)\n",
        "\n",
        "if torch.cuda.is_available() and not args.cuda:\n",
        "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "np.random.seed(cfg.RNG_SEED)\n",
        "if args.dataset == \"uav\":\n",
        "    args.imdb_name = \"uav_2017_trainval\"\n",
        "    args.imdbval_name = \"uav_2017_test\"\n",
        "    args.set_cfgs = ['ANCHOR_SCALES', '[8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]', 'MAX_NUM_GT_BOXES', '20']\n",
        "\n",
        "\n",
        "args.cfg_file = \"cfgs/{}_ls.yml\".format(args.net) if args.large_scale else \"cfgs/{}.yml\".format(args.net)\n",
        "\n",
        "if args.cfg_file is not None:\n",
        "    cfg_from_file(args.cfg_file)\n",
        "if args.set_cfgs is not None:\n",
        "    cfg_from_list(args.set_cfgs)\n",
        "\n",
        "print('Using config:')\n",
        "pprint.pprint(cfg)\n",
        "\n",
        "cfg.TRAIN.USE_FLIPPED = False\n",
        "imdb, roidb, ratio_list, ratio_index = combined_roidb(args.imdbval_name, False)\n",
        "imdb.competition_mode(on=True)\n",
        "imdb.set_epoch(args.checkepoch)\n",
        "imdb.set_ckpt(args.checkpoint)\n",
        "print('{:d} roidb entries'.format(len(roidb)))\n",
        "\n",
        "nuisance_type =\"Baseline\"\n",
        "\n",
        "model_dir = os.path.join(args.model_dir)\n",
        "if not os.path.exists(model_dir):\n",
        "    raise Exception('There is no input directory for loading network from ' + model_dir)\n",
        "\n",
        "load_name = os.path.join(model_dir,'faster_rcnn_{}_{}_{}.pth'.format(args.checksession, args.checkepoch, args.checkpoint))\n",
        "\n",
        "# initilize the network here.\n",
        "if args.net == 'vgg16':\n",
        "    fasterRCNN = vgg16(imdb.classes, pretrained=False, class_agnostic=args.class_agnostic)\n",
        "elif args.net == 'res101':\n",
        "    fasterRCNN = resnet(imdb.classes, 101, pretrained=False, class_agnostic=args.class_agnostic)\n",
        "elif args.net == 'res50':\n",
        "    fasterRCNN = resnet(imdb.classes, 50, pretrained=False, class_agnostic=args.class_agnostic)\n",
        "elif args.net == 'res152':\n",
        "    fasterRCNN = resnet(imdb.classes, 152, pretrained=False, class_agnostic=args.class_agnostic)\n",
        "else:\n",
        "    print(\"network is not defined\")\n",
        "    pdb.set_trace()\n",
        "\n",
        "fasterRCNN.create_architecture()\n",
        "\n",
        "print(\"load checkpoint %s\" % (load_name))\n",
        "checkpoint = torch.load(load_name)\n",
        "\n",
        "for key in ['RCNN_angle_score.weight', 'RCNN_angle_score.bias',\n",
        "            'RCNN_altitude_score.weight', 'RCNN_altitude_score.bias',\n",
        "            'RCNN_weather_score.weight', 'RCNN_weather_score.bias'\n",
        "            ]:\n",
        "    if key in checkpoint['model'].keys():\n",
        "        del checkpoint['model'][key]\n",
        "\n",
        "model_dict = fasterRCNN.state_dict()\n",
        "model_dict.update(checkpoint['model'])\n",
        "fasterRCNN.load_state_dict(model_dict)\n",
        "\n",
        "# fasterRCNN.load_state_dict(checkpoint['model'])\n",
        "if 'pooling_mode' in checkpoint.keys():\n",
        "    cfg.POOLING_MODE = checkpoint['pooling_mode']\n",
        "\n",
        "print('load model successfully!')\n",
        "# initilize the tensor holder here.\n",
        "im_data = torch.FloatTensor(1)\n",
        "im_info = torch.FloatTensor(1)\n",
        "meta_data = torch.FloatTensor(1)\n",
        "num_boxes = torch.LongTensor(1)\n",
        "gt_boxes = torch.FloatTensor(1)\n",
        "\n",
        "# ship to cuda\n",
        "if args.cuda:\n",
        "    im_data = im_data.cuda()\n",
        "    im_info = im_info.cuda()\n",
        "    meta_data = meta_data.cuda()\n",
        "    num_boxes = num_boxes.cuda()\n",
        "    gt_boxes = gt_boxes.cuda()\n",
        "\n",
        "# make variable\n",
        "im_data = Variable(im_data)\n",
        "im_info = Variable(im_info)\n",
        "meta_data = Variable(meta_data)\n",
        "num_boxes = Variable(num_boxes)\n",
        "gt_boxes = Variable(gt_boxes)\n",
        "\n",
        "if args.cuda:\n",
        "    cfg.CUDA = True\n",
        "\n",
        "if args.cuda:\n",
        "    fasterRCNN.cuda()\n",
        "\n",
        "start = time.time()\n",
        "max_per_image = 100\n",
        "\n",
        "vis = args.vis\n",
        "\n",
        "if vis:\n",
        "    thresh = 0.05\n",
        "else:\n",
        "    thresh = 0.0\n",
        "\n",
        "save_name = 'faster_rcnn_10'\n",
        "num_images = len(imdb.image_index)\n",
        "all_boxes = [[[] for _ in xrange(num_images)]\n",
        "              for _ in xrange(imdb.num_classes)]\n",
        "\n",
        "output_dir = get_output_dir(imdb, save_name)\n",
        "dataset = roibatchLoader(roidb, ratio_list, ratio_index, 1, \\\n",
        "                          imdb.num_classes, training=False, normalize=False)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1,\n",
        "                                          shuffle=False, num_workers=4,\n",
        "                                          pin_memory=True)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "\n",
        "_t = {'im_detect': time.time(), 'misc': time.time()}\n",
        "det_file = os.path.join(output_dir, 'detections.pkl')\n",
        "\n",
        "fasterRCNN.eval()\n",
        "empty_array = np.transpose(np.array([[], [], [], [], []]), (1, 0))\n",
        "for i in range(num_images):\n",
        "    data = next(data_iter)\n",
        "    im_data.resize_(data[0].size()).copy_(data[0])\n",
        "    im_info.resize_(data[1].size()).copy_(data[1])\n",
        "    gt_boxes.resize_(data[2].size()).copy_(data[2])\n",
        "    num_boxes.resize_(data[3].size()).copy_(data[3])\n",
        "\n",
        "    det_tic = time.time()\n",
        "    rois, cls_prob, bbox_pred, _, _, _, _, _ = fasterRCNN(im_data, im_info, gt_boxes, num_boxes)\n",
        "\n",
        "    scores = cls_prob.data\n",
        "    boxes = rois.data[:, :, 1:5]\n",
        "\n",
        "    if cfg.TEST.BBOX_REG:\n",
        "        # Apply bounding-box regression deltas\n",
        "        box_deltas = bbox_pred.data\n",
        "        if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n",
        "            # Optionally normalize targets by a precomputed mean and stdev\n",
        "            if args.class_agnostic:\n",
        "                box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n",
        "                              + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n",
        "                box_deltas = box_deltas.view(1, -1, 4)\n",
        "            else:\n",
        "                box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n",
        "                              + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n",
        "                box_deltas = box_deltas.view(1, -1, 4 * len(imdb.classes))\n",
        "\n",
        "        pred_boxes = bbox_transform_inv(boxes, box_deltas, 1)\n",
        "        pred_boxes = clip_boxes(pred_boxes, im_info.data, 1)\n",
        "    else:\n",
        "        # Simply repeat the boxes, once for each class\n",
        "        pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n",
        "\n",
        "    pred_boxes /= data[1][0][2].item()\n",
        "\n",
        "    scores = scores.squeeze()\n",
        "    pred_boxes = pred_boxes.squeeze()\n",
        "    det_toc = time.time()\n",
        "    detect_time = det_toc - det_tic\n",
        "    misc_tic = time.time()\n",
        "    if vis:\n",
        "        im = cv2.imread(imdb.image_path_at(i))\n",
        "        im2show = np.copy(im)\n",
        "    for j in xrange(1, imdb.num_classes):\n",
        "        inds = torch.nonzero(scores[:, j] > thresh).view(-1)\n",
        "        # if there is det\n",
        "        if inds.numel() > 0:\n",
        "            cls_scores = scores[:, j][inds]\n",
        "            _, order = torch.sort(cls_scores, 0, True)\n",
        "            if args.class_agnostic:\n",
        "                cls_boxes = pred_boxes[inds, :]\n",
        "            else:\n",
        "                cls_boxes = pred_boxes[inds][:, j * 4:(j + 1) * 4]\n",
        "\n",
        "            cls_dets = torch.cat((cls_boxes, cls_scores.unsqueeze(1)), 1)\n",
        "            # cls_dets = torch.cat((cls_boxes, cls_scores), 1)\n",
        "            cls_dets = cls_dets[order]\n",
        "            #keep = nms(cls_dets, cfg.TEST.NMS)\n",
        "            keep = nms(cls_boxes[order, :], cls_scores[order], cfg.TEST.NMS)\n",
        "            cls_dets = cls_dets[keep.view(-1).long()]\n",
        "            if vis:\n",
        "                im2show = vis_detections(im2show, imdb.classes[j], cls_dets.cpu().numpy(), 0.3)\n",
        "            all_boxes[j][i] = cls_dets.cpu().numpy()\n",
        "        else:\n",
        "            all_boxes[j][i] = empty_array\n",
        "\n",
        "    # Limit to max_per_image detections *over all classes*\n",
        "    if max_per_image > 0:\n",
        "        image_scores = np.hstack([all_boxes[j][i][:, -1]\n",
        "                                  for j in xrange(1, imdb.num_classes)])\n",
        "        all_boxes[j][i] = np.concatenate(\n",
        "            (all_boxes[j][i], np.tile(meta_data.cpu().numpy(), (len(image_scores), 1))),\n",
        "            axis=1)\n",
        "        if len(image_scores) > max_per_image:\n",
        "            image_thresh = np.sort(image_scores)[-max_per_image]\n",
        "            for j in xrange(1, imdb.num_classes):\n",
        "                keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n",
        "                all_boxes[j][i] = all_boxes[j][i][keep, :]\n",
        "\n",
        "    misc_toc = time.time()\n",
        "    nms_time = misc_toc - misc_tic\n",
        "\n",
        "    sys.stdout.write('im_detect: {:d}/{:d} {:.3f}s {:.3f}s   \\r' \\\n",
        "                      .format(i + 1, num_images, detect_time, nms_time))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    if vis:\n",
        "        cv2.imwrite('result.png', im2show)\n",
        "        pdb.set_trace()\n",
        "\n",
        "with open(det_file, 'wb') as f:\n",
        "    pickle.dump(all_boxes, f)\n",
        "\n",
        "print('Evaluating detections')\n",
        "\n",
        "cachedir = 'data/VOCdevkit2007/annotations_cache/data/VOCdevkit2007/UAV2017/ImageSets/Main'\n",
        "if not os.path.isdir(cachedir):\n",
        "    os.makedirs(cachedir)\n",
        "\n",
        "imdb.evaluate_detections(all_boxes, output_dir, nuisance_type=nuisance_type, baseline_method=args.is_baseline_method, ovthresh=args.ovthresh)\n",
        "\n",
        "end = time.time()\n",
        "print(\"test time: %0.4fs\" % (end - start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYFj-a28ZFX9"
      },
      "outputs": [],
      "source": [
        "!python demo.py --net vgg16 \\\n",
        "             --dataset uav\\\n",
        "              --checkepoch 2\\\n",
        "               --checkpoint 110 \\\n",
        "               --cuda --load_dir \"/content/gdrive/My Drive/assignment3-part2/faster-rcnn.pytorch/models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8e_86AfjP_7"
      },
      "outputs": [],
      "source": [
        "#--------------------------------------------------\n",
        "#    You need to get detection result images here\n",
        "#--------------------------------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# read the image for local directory (same with this .ipynb) \n",
        "img1 = cv2.imread('images/img0.jpg')\n",
        "img2 = cv2.imread('images/img2.jpg')\n",
        "\n",
        "img1_det = cv2.imread('images/img0_det.jpg')\n",
        "img2_det = cv2.imread('images/img2_det.jpg')\n",
        "\n",
        "# Be sure to convert the color space of the image from\n",
        "# BGR (Opencv) to RGB (Matplotlib) before you show a \n",
        "# color image read from OpenCV\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
        "plt.title('train image')\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(cv2.cvtColor(img1_det, cv2.COLOR_BGR2RGB))\n",
        "plt.title('train image (detection)')\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
        "plt.title('test image')\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(cv2.cvtColor(img2_det, cv2.COLOR_BGR2RGB))\n",
        "plt.title('test image (detection)')\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4X3bWAMLhvv"
      },
      "source": [
        "### Discussion:\n",
        "(10 points) \n",
        "\n",
        "1. Compare your detection results with ground-truth. Is it good or bad? In what aspects. \n",
        "\n",
        "[your answer]: \n",
        "\n",
        "2. Which hyper-parameters during training stage or properties of dataset would affect the detection results most by your understanding?\n",
        "\n",
        "[your answer]:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UGefIYpYYNF"
      },
      "source": [
        "## Submission guidelines\n",
        "---\n",
        "Extract the downloaded .zip file to a folder of your preference. The input and output paths are predefined and **DO NOT** change them, (we assume that 'Surname_Givenname_assignment3_part2' is your working directory, and all the paths are relative to this directory).  The image read and write functions are already written for you. All you need to do is to fill in the blanks as indicated to generate proper outputs. **DO NOT** zip and upload the dataset on canvas due to size limit.\n",
        "\n",
        "When submitting your .zip file through canvas, please\n",
        "-- name your .zip file as **Surname_Givenname_assignment3_part2.zip**.\n",
        "\n",
        "This zip file should include:\n",
        "```\n",
        "Surname_Givenname_UNCCID_assignment3_part1  \n",
        "        |---Surname_Givenname_UNCCID_assignment3_part2.ipynb\n",
        "        |---Surname_Givenname_UNCCID_assignment3_part2.pdf\n",
        "```\n",
        "\n",
        "For instance, student Yann Lecun should submit a zip file named \"Lecun_Yann_111134567_assignment2_part2.zip\" for assignment2_part2 in this structure:\n",
        "```\n",
        "Lecun_Yann_111134567_assignment3_part1\n",
        "        |---Lecun_Yann_111134567_assignment3_part2.ipynb\n",
        "        |---Lecun_Yann_111134567_assignment3_part2.pdf\n",
        "```\n",
        "\n",
        "Then right click this folder, click ***Get shareable link***, in the People textfield, enter TA's emails: ***psingire@uncc.edu*** and ***kchiguru@uncc.edu***. Make sure that TAs who have the link **can edit**, ***not just*** **can view**, and also **uncheck** the **Notify people** box.\n",
        "\n",
        "Note that in google colab, we will only grade the version of the code right before the timestamp of the submission made in canvas. \n",
        "\n",
        "Extract the downloaded .zip file to a folder of your preference. The input and output paths are predefined and **DO NOT** change them, (we assume that 'Surname_Givenname_UNCCID_assignment3' is your working directory, and all the paths are relative to this directory).  The image read and write functions are already written for you. All you need to do is to fill in the blanks as indicated to generate proper outputs.\n",
        "\n",
        "\n",
        "-- DO NOT change the folder structure, please just fill in the blanks. <br>\n",
        "\n",
        "You are encouraged to post and answer questions on Canvas. Please ask questions on Canvas and send emails only for personal issues.\n",
        "\n",
        "If you alter the folder structures, the grading of your homework will be significantly delayed and possibly penalized.\n",
        "\n",
        "Be aware that your code will undergo plagiarism check both vertically and horizontally. Please do your own work.\n",
        "\n",
        "Late submission penalty: <br>\n",
        "There will be a 10% penalty per day for late submission. However, you will have THREE days throughout the whole semester to submit late without penalty. Note that the grace period is calculated by days instead of hours. If you submit the homework one minute after the deadline, one late day will be counted. Likewise, if you submit one minute after the deadline, the 10% penaly will be imposed if not using the grace period.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "machine_shape": "hm",
      "name": "CSE527-20S-HW5-P2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}